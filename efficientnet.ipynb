{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f",
   "metadata": {},
   "source": [
    "# [모의 캐글-의료] 흉부 CT 코로나 감염 여부 분류\n",
    "- 이미지 binary 분류 과제\n",
    "- 담당: 이녕민M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4325d39-6344-4116-b343-df51696905ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get install -y python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980a049e-d1d1-4b4b-8312-c236c2a4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f475804-13db-484c-a348-f01580e80a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1a1ae4-d045-4fc1-9535-68c69895431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b906e65-bfe6-4766-952e-990a994ad466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb66de-b8cb-4891-9fd7-812ca280b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Arguments & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9c63836-0a7a-48f7-a5ea-e16b617fec65",
   "metadata": {},
   "source": [
    "# 데이터 디렉토리 구조\n",
    "\n",
    "data/  \n",
    "  \\_train/  \n",
    "    \\_0.png  \n",
    "    \\_1.png  \n",
    "    \\_...  \n",
    "  \\_test/  \n",
    "    \\_0.png  \n",
    "    \\_1.png  \n",
    "    \\_...  \n",
    "  \\_train.csv  \n",
    "  \\_sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50711fda-b652-448a-9ba4-aa5316c966cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/USER/daeyeong\")  # 기준 경로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "### 데이터 디렉토리 설정 ###\n",
    "DATA_DIR= 'data'\n",
    "NUM_CLS = 2\n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "INPUT_SHAPE = 128\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {},
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6116228-b14e-4172-a535-918d5a7f1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# tem = pd.read_csv(os.path.join(DATA_DIR, 'train1.csv'))\n",
    "# X_train, X_val, y_train, y_val = train_test_split(tem, tem['COVID'], test_size=0.1, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Dataset split\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train1.csv'))\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['COVID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46072b00-108a-42fd-a7fe-f55c529e6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmentables\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19427235-4270-4974-864c-5bb09cc3201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augment시 사용\n",
    "eq_img_list = []\n",
    "class Augmentation:\n",
    "    def __init__(self, data_dir, mode, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        self.img = None\n",
    "    \n",
    "        #이미지크기 저장공간\n",
    "\n",
    "        self.db = self.data_loader()\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "       \n",
    "        return db\n",
    "    def Equalization(self):\n",
    "    #    // for cnt in tqdm(range(646)):\n",
    "        for i in tqdm(range(646)):\n",
    "            # img = eq_img_list\n",
    "            db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "            data = copy.deepcopy(db.loc[i])\n",
    "            cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']))\n",
    "            cvimg_yuv = cv2.cvtColor(cvimg, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "            img_clahe = cvimg_yuv.copy()\n",
    "            clahe = cv2.createCLAHE(clipLimit = 3.0 , tileGridSize=(8,8))\n",
    "            img_clahe[:,:,0] = clahe.apply(img_clahe[:,:,0])\n",
    "            img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "                # img_eq = cv2_imshow(img_clahe)\n",
    "            # cv2.waitKey()\n",
    "            cv2.destroyAllWindows()\n",
    "            eq_img_list.append(img_clahe)\n",
    "            # os.makedirs('/content/drive/MyDrive/이어드림/project2/data/aug_img', exist_ok=True)\n",
    "            cv2.imwrite(f'{self.data_dir}/train/{i + 646}.png', img_clahe)\n",
    "            self.img = eq_img_list\n",
    "        return eq_img_list\n",
    "\n",
    "    def augmentation(self):\n",
    "        \n",
    "        for i in tqdm(range(646)):\n",
    "            \n",
    "            db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "            data = copy.deepcopy(db.loc[i])\n",
    "            sss =cv2.imread(os.path.join(self.data_dir,'train',data['file_name']))\n",
    "            \n",
    "            # cv2_imshow(sss)\n",
    "            augmentation_resize = iaa.Sequential([\n",
    "                            iaa.Resize({\"height\":384,\"width\":384},interpolation=\"cubic\")\n",
    "                        ])\n",
    "\n",
    "\n",
    "            image_aug = augmentation_resize(image=sss)\n",
    "            # cv2_imshow(image_aug)\n",
    "            # sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "            for j in (range(3)):\n",
    "                augmentation_resize = iaa.Sequential([\n",
    "                                    # iaa.Affine(translate_percent={\"x\":(-0.5,0.5),\"y\":(-0.5,0.5)},rotate=(-2,2),scale=(0.5,2)),\n",
    "                                    iaa.LinearContrast((0.75,1.45)),\n",
    "                                    iaa.GaussianBlur((0.0,1.5)),\n",
    "                                    iaa.Fliplr(0.5),\n",
    "                                    # iaa.Flipud(0.3),\n",
    "                                    # iaa.Affine(translate_px={\"x\": (-20, 20), \"y\": (-20, 20)}),\n",
    "                                    # iaa.PerspectiveTransform(scale=(0.01, 0.15)),\n",
    "                                    # iaa.PiecewiseAffine(scale=(0.01, 0.05)),\n",
    "                                    # iaa.Sharpen(alpha=(0, 0.3), lightness=(0.75, 1.00)),\n",
    "                                    # iaa.Emboss(alpha=(0, 1.0), strength=(0, 1.5)),\n",
    "                                    ])\n",
    "                    \n",
    "                image_augs = augmentation_resize(image=image_aug)\n",
    "            # cv2_imshow(image_augs)\n",
    "                # os.makedirs('/content/drive/MyDrive/이어드림/project2/data/augment{}'.format(j), exist_ok=True)\n",
    "                cv2.imwrite(f'{self.data_dir}/train/{(i)+(646)+(646*j)}.png', image_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c557a0f-dce8-48e2-8f72-f799f58039c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 3/646 [00:00<00:23, 27.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "이미w지를 불러오는 중입니다.\n",
      "Loading train dataset..\n",
      "******************************\n",
      "Equalization중 입니다\n",
      "******************************\n",
      "Equalization 성공\n",
      "Augmentation중 입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 646/646 [00:17<00:00, 37.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#data augm\n",
    "if __name__ == '__main__': \n",
    "    temp = Augmentation(DATA_DIR,'train', INPUT_SHAPE)\n",
    "    print('이미w지를 불러오는 중입니다.')\n",
    "    temp.data_loader()\n",
    "    print('*' * 30)\n",
    "    print('Equalization중 입니다')\n",
    "    # temp.Equalization()\n",
    "    print('*' * 30)\n",
    "    print('Equalization 성공')\n",
    "    print('Augmentation중 입니다')\n",
    "    temp.augmentation()\n",
    "\n",
    "    print('*' * 30)\n",
    "    del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2c47f77-f033-4222-a0d6-c9c4c8d8f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ddd37d7-7408-4627-803c-3dd0a753eba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['COVID'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85557a63-cbde-47b6-826f-80cb051692f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "645+(646*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d78dd843-5bb3-4d97-b242-6f6ae8e986a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = []\n",
    "COVID = []\n",
    "for i in range(2584):\n",
    "    file_name.append(f'{i}.png')\n",
    "    COVID.append(db['COVID'][i % 646])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f879845-10a2-4e14-9d2c-ad0f59771af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':file_name, 'COVID':COVID})\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('train1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584f00b-a211-4774-b882-a079e8ff836d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b27520-c82c-4ec8-ae0b-119a79167f09",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3a46698a-2bf4-4895-a96a-92452552c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "\n",
    "class Efficientnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Efficientnet, self).__init__()\n",
    "        self.efficientnet = timm.create_model('efficientnetv2_s', pretrained=False, num_classes=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet(x)\n",
    "        \n",
    "        output = self.softmax(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056905-1f77-4579-a260-07bb1056f6db",
   "metadata": {},
   "source": [
    "## Utils\n",
    "### EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b4c3315-ebca-4e6b-a8f2-1281ccd0bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가, 감소 시 0으로 리셋\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "        self.save_model = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            return None\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "        \n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" epoch에 대한 학습 및 검증 절차 정의\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, model, device, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, F1-Macro: {f1}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, F1-Macro: {f1}'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    \n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    f1 = f1_score(y_answer, y_pred, average='macro')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729c079-9d85-49ce-857f-320b0c56a3a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train\n",
    "### 학습을 위한 객체 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19610a4-ad7c-44a0-80cd-9734b5015100",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7cea68f0-dfad-47ce-a8ca-00ea01988886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n",
      "Train set samples: 2325 Val set samples: 259\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8dae0-8e32-4ac0-a585-858a7095d2a4",
   "metadata": {},
   "source": [
    "#### Load model and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb4d52e1-752a-40d5-9b34-c06d3dbdd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = Efficientnet(NUM_CLS)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# # Save Initial Model\n",
    "# torch.save(model.state_dict(), 'initial.pt')\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(loss_fn, model, DEVICE, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa8aef-b984-4133-b6b2-e1c85900f724",
   "metadata": {},
   "source": [
    "### epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dcc35f70-25fc-48e1-92f8-8633b3b8be80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 1.3299112369616826, Acc: 0.5221505376344086, F1-Macro: 0.49779633676103796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 1/60 [04:13<4:09:01, 253.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val loss: 1.021317020058632, Acc: 0.5173745173745173, F1-Macro: 0.5043403708069875\n",
      "Epoch 1, Train loss: 1.447876963350508, Acc: 0.49376344086021506, F1-Macro: 0.491169600092524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 2/60 [08:35<4:07:22, 255.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val loss: 2.6491337716579437, Acc: 0.4864864864864865, F1-Macro: 0.4484685463598956\n",
      "Early stopping counter 1/20\n",
      "Epoch 2, Train loss: 1.7274970942073398, Acc: 0.46365591397849465, F1-Macro: 0.43586946179579633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% 3/60 [12:58<4:05:16, 258.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val loss: 2.424583673477173, Acc: 0.49034749034749037, F1-Macro: 0.4436995574069253\n",
      "Early stopping counter 2/20\n",
      "Epoch 3, Train loss: 4.198280284802119, Acc: 0.46365591397849465, F1-Macro: 0.4157124780006082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% 4/60 [17:23<4:02:39, 259.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val loss: 5.228681682841852, Acc: 0.4864864864864865, F1-Macro: 0.4228533132277792\n",
      "Early stopping counter 3/20\n",
      "Epoch 4, Train loss: 4.302237086825901, Acc: 0.4696774193548387, F1-Macro: 0.41547430949720054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% 5/60 [21:32<3:55:27, 256.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Val loss: 4.4040690660476685, Acc: 0.4942084942084942, F1-Macro: 0.4247469524084027\n",
      "Early stopping counter 4/20\n",
      "Epoch 5, Train loss: 4.05571730600463, Acc: 0.47139784946236557, F1-Macro: 0.41323577698554825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 6/60 [25:45<3:50:05, 255.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Val loss: 4.88250333070755, Acc: 0.4980694980694981, F1-Macro: 0.42738095238095236\n",
      "Early stopping counter 5/20\n",
      "Epoch 6, Train loss: 3.815714564588335, Acc: 0.46580645161290324, F1-Macro: 0.40246154431922027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12% 7/60 [29:23<3:35:46, 244.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Val loss: 3.904910087585449, Acc: 0.4980694980694981, F1-Macro: 0.42738095238095236\n",
      "Early stopping counter 6/20\n",
      "Epoch 7, Train loss: 3.394385861025916, Acc: 0.47053763440860213, F1-Macro: 0.4079391173807516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% 8/60 [33:38<3:34:38, 247.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Val loss: 3.6305618584156036, Acc: 0.49034749034749037, F1-Macro: 0.4185714285714286\n",
      "Early stopping counter 7/20\n",
      "Epoch 8, Train loss: 3.0016906758149466, Acc: 0.4666666666666667, F1-Macro: 0.3964548473698578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15% 9/60 [38:06<3:35:40, 253.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Val loss: 4.270425587892532, Acc: 0.5019305019305019, F1-Macro: 0.42637136234869943\n",
      "Early stopping counter 8/20\n",
      "Epoch 9, Train loss: 2.637688837117619, Acc: 0.469247311827957, F1-Macro: 0.3981717662102725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17% 10/60 [42:30<3:33:59, 256.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Val loss: 2.49759903550148, Acc: 0.4980694980694981, F1-Macro: 0.41618116243584413\n",
      "Early stopping counter 9/20\n",
      "Epoch 10, Train loss: 2.281357364522086, Acc: 0.4735483870967742, F1-Macro: 0.4062016435557292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18% 11/60 [46:48<3:30:01, 257.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Val loss: 3.229373872280121, Acc: 0.5019305019305019, F1-Macro: 0.4300117713291366\n",
      "Early stopping counter 10/20\n",
      "Epoch 11, Train loss: 2.1037089940574436, Acc: 0.48, F1-Macro: 0.4217370938088423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 12/60 [51:11<3:27:02, 258.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Val loss: 2.406519740819931, Acc: 0.4980694980694981, F1-Macro: 0.42738095238095236\n",
      "Early stopping counter 11/20\n",
      "Epoch 12, Train loss: 1.962478044960234, Acc: 0.48258064516129034, F1-Macro: 0.4499848483924843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22% 13/60 [55:33<3:23:26, 259.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Val loss: 2.2889160364866257, Acc: 0.49034749034749037, F1-Macro: 0.448936170212766\n",
      "Early stopping counter 12/20\n",
      "Epoch 13, Train loss: 1.9364389065239165, Acc: 0.5083870967741936, F1-Macro: 0.4976441510200803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23% 14/60 [59:55<3:19:42, 260.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Val loss: 2.7218008935451508, Acc: 0.5173745173745173, F1-Macro: 0.5173745173745173\n",
      "Early stopping counter 13/20\n",
      "Epoch 14, Train loss: 1.6767427954408858, Acc: 0.5105376344086021, F1-Macro: 0.5095751337901501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25% 15/60 [1:03:38<3:06:56, 249.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Val loss: 4.7681746780872345, Acc: 0.5212355212355212, F1-Macro: 0.5186450839328538\n",
      "Early stopping counter 14/20\n",
      "Epoch 15, Train loss: 2.6101172500186496, Acc: 0.5131182795698924, F1-Macro: 0.5117959912327694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% 16/60 [1:08:02<3:06:04, 253.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Val loss: 9.141071557998657, Acc: 0.47876447876447875, F1-Macro: 0.4492052740189978\n",
      "Early stopping counter 15/20\n",
      "Epoch 16, Train loss: 6.789684918191698, Acc: 0.4666666666666667, F1-Macro: 0.43801169590643274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% 17/60 [1:12:10<3:00:33, 251.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Val loss: 8.525649428367615, Acc: 0.46332046332046334, F1-Macro: 0.4183724574696674\n",
      "Early stopping counter 16/20\n",
      "Epoch 17, Train loss: 6.256861183378431, Acc: 0.4726881720430108, F1-Macro: 0.4420291273538738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% 18/60 [1:16:32<2:58:25, 254.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Val loss: 7.579774975776672, Acc: 0.4671814671814672, F1-Macro: 0.4184131736526946\n",
      "Early stopping counter 17/20\n",
      "Epoch 18, Train loss: 5.567378633552128, Acc: 0.4735483870967742, F1-Macro: 0.4434126405452514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32% 19/60 [1:21:02<2:57:23, 259.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Val loss: 6.753124475479126, Acc: 0.46332046332046334, F1-Macro: 0.4156156156156156\n",
      "Early stopping counter 18/20\n",
      "Epoch 19, Train loss: 4.9566852119233875, Acc: 0.47483870967741937, F1-Macro: 0.4458235181697735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% 20/60 [1:25:33<2:55:23, 263.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Val loss: 4.873050963506103, Acc: 0.47104247104247104, F1-Macro: 0.4212054481689911\n",
      "Early stopping counter 19/20\n",
      "Epoch 20, Train loss: 4.428588718175888, Acc: 0.47397849462365593, F1-Macro: 0.4439849238124569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% 20/60 [1:30:07<3:00:15, 270.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Val loss: 5.469532787799835, Acc: 0.4749034749034749, F1-Macro: 0.43223726627981945\n",
      "Early stopping counter 20/20\n",
      "Early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if early_stopper.save_model:\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, 'best.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53514a-e83f-4795-9589-640f26cc2993",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference\n",
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6729cfde-c4b3-4d36-938e-f8bb8d8afef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = 'best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ced90de9-50ec-4e18-9f42-5a1b493941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'sample_submission.csv'))\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'test',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efd72b-172d-4e34-a1dd-65ed8c745b58",
   "metadata": {},
   "source": [
    "### 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "16a090ea-bb34-4d3d-a127-b1190e8c416c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.2919e-02, 9.0708e-01],\n",
      "        [4.4915e-01, 5.5085e-01],\n",
      "        [1.0805e-01, 8.9195e-01],\n",
      "        [2.0093e-01, 7.9907e-01],\n",
      "        [5.8540e-02, 9.4146e-01],\n",
      "        [1.1323e-01, 8.8677e-01],\n",
      "        [2.0215e-01, 7.9785e-01],\n",
      "        [1.3539e-01, 8.6461e-01],\n",
      "        [1.8725e-01, 8.1275e-01],\n",
      "        [3.5899e-01, 6.4101e-01],\n",
      "        [1.1026e-01, 8.8974e-01],\n",
      "        [1.7864e-01, 8.2136e-01],\n",
      "        [1.4950e-01, 8.5050e-01],\n",
      "        [8.9599e-01, 1.0401e-01],\n",
      "        [1.2247e-01, 8.7753e-01],\n",
      "        [1.5026e-01, 8.4974e-01],\n",
      "        [1.1138e-01, 8.8862e-01],\n",
      "        [1.5254e-01, 8.4746e-01],\n",
      "        [2.0075e-01, 7.9925e-01],\n",
      "        [1.2714e-01, 8.7286e-01],\n",
      "        [2.1569e-01, 7.8431e-01],\n",
      "        [1.3249e-01, 8.6751e-01],\n",
      "        [1.1024e-01, 8.8976e-01],\n",
      "        [1.2237e-01, 8.7763e-01],\n",
      "        [9.9984e-01, 1.6165e-04],\n",
      "        [1.7938e-01, 8.2062e-01],\n",
      "        [1.1954e-01, 8.8046e-01],\n",
      "        [2.3681e-01, 7.6319e-01],\n",
      "        [9.9054e-01, 9.4551e-03],\n",
      "        [1.4107e-01, 8.5893e-01],\n",
      "        [6.2539e-02, 9.3746e-01],\n",
      "        [3.2066e-01, 6.7934e-01],\n",
      "        [2.1696e-01, 7.8304e-01],\n",
      "        [1.9217e-01, 8.0783e-01],\n",
      "        [2.0586e-01, 7.9414e-01],\n",
      "        [2.4394e-01, 7.5606e-01],\n",
      "        [3.5980e-01, 6.4020e-01],\n",
      "        [1.6742e-01, 8.3258e-01],\n",
      "        [4.1709e-01, 5.8291e-01],\n",
      "        [1.3464e-01, 8.6536e-01],\n",
      "        [1.1491e-01, 8.8509e-01],\n",
      "        [3.7649e-01, 6.2351e-01],\n",
      "        [1.7251e-01, 8.2749e-01],\n",
      "        [1.9689e-01, 8.0311e-01],\n",
      "        [1.4565e-01, 8.5435e-01],\n",
      "        [1.8378e-01, 8.1622e-01],\n",
      "        [2.6563e-01, 7.3437e-01],\n",
      "        [1.4086e-01, 8.5914e-01],\n",
      "        [1.9126e-01, 8.0874e-01],\n",
      "        [1.3409e-01, 8.6591e-01],\n",
      "        [1.5218e-01, 8.4782e-01],\n",
      "        [1.6014e-01, 8.3986e-01],\n",
      "        [1.5308e-01, 8.4692e-01],\n",
      "        [1.2832e-01, 8.7168e-01],\n",
      "        [1.2588e-01, 8.7412e-01],\n",
      "        [1.0653e-01, 8.9347e-01],\n",
      "        [1.4155e-01, 8.5845e-01],\n",
      "        [1.1946e-01, 8.8054e-01],\n",
      "        [4.1984e-01, 5.8016e-01],\n",
      "        [1.1750e-01, 8.8250e-01],\n",
      "        [1.3756e-01, 8.6244e-01],\n",
      "        [1.6081e-01, 8.3919e-01],\n",
      "        [2.8141e-01, 7.1859e-01],\n",
      "        [1.4165e-01, 8.5835e-01]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:10,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4210, 0.5790],\n",
      "        [0.8189, 0.1811],\n",
      "        [0.9514, 0.0486],\n",
      "        [0.2170, 0.7830],\n",
      "        [0.2356, 0.7644],\n",
      "        [0.1252, 0.8748],\n",
      "        [0.1045, 0.8955],\n",
      "        [0.1260, 0.8740],\n",
      "        [0.1887, 0.8113],\n",
      "        [0.1993, 0.8007],\n",
      "        [0.4402, 0.5598],\n",
      "        [0.7066, 0.2934],\n",
      "        [0.1330, 0.8670],\n",
      "        [0.6236, 0.3764],\n",
      "        [0.0816, 0.9184],\n",
      "        [0.2396, 0.7604],\n",
      "        [0.9286, 0.0714],\n",
      "        [0.2138, 0.7862],\n",
      "        [0.1890, 0.8110],\n",
      "        [0.1721, 0.8279],\n",
      "        [0.1084, 0.8916],\n",
      "        [0.5921, 0.4079],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.1341, 0.8659],\n",
      "        [0.1522, 0.8478],\n",
      "        [0.1230, 0.8770],\n",
      "        [0.8489, 0.1511],\n",
      "        [0.1353, 0.8647],\n",
      "        [0.3471, 0.6529],\n",
      "        [0.1074, 0.8926],\n",
      "        [0.1474, 0.8526],\n",
      "        [0.0958, 0.9042],\n",
      "        [0.3897, 0.6103],\n",
      "        [0.7763, 0.2237],\n",
      "        [0.1601, 0.8399],\n",
      "        [0.0771, 0.9229]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        print(pred)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst.extend(pred[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056169d1-64a8-4b81-8daf-722b029cf2b9",
   "metadata": {},
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f133cd86-b87b-4f8b-ae0e-c240655ae9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':file_lst, 'COVID':pred_lst})\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32903131-b832-4fee-a246-4dd55cda4625",
   "metadata": {},
   "source": [
    "앙상블을 위한 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a5b7ae-2810-42c4-9335-ae1c6b6e2746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>COVID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  COVID\n",
       "0      0.png      0\n",
       "1      1.png      0\n",
       "2      2.png      0\n",
       "3      3.png      0\n",
       "4      4.png      0\n",
       "..       ...    ...\n",
       "95    95.png      0\n",
       "96    96.png      1\n",
       "97    97.png      1\n",
       "98    98.png      1\n",
       "99    99.png      0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        print(pred)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.tolist())\n",
    "df = pd.DataFrame(pred_lst)\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('prediction111.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34aabb9-b3aa-4bd0-af7c-68174c21f220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
